import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# Generating synthetic data
np.random.seed(42)
num_samples = 100
negative_samples = np.random.multivariate_normal(mean=[0, 3], cov=[[1, 0.5], [0.5, 1]],
size=num_samples)
positive_samples = np.random.multivariate_normal(mean=[3, 0], cov=[[1, 0.5], [0.5, 1]],
size=num_samples)
inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)
labels = np.vstack((np.zeros((num_samples, 1), dtype='float32'),
np.ones((num_samples, 1), dtype='float32')))
# Shuffle the data
indices = np.random.permutation(len(inputs))
inputs = inputs[indices]
labels = labels[indices]
# Visualizing the data
plt.scatter(inputs[:, 0], inputs[:, 1], c=labels.flatten(), cmap='coolwarm', edgecolors='k')
plt.title('Linear Classification - Synthetic Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
# Create TensorFlow dataset
dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))
# Model parameters
learning_rate = 0.01
epochs = 100
batch_size = 16
# Define the linear classifier model
model = tf.keras.Sequential([
tf.keras.layers.Dense(1, activation='sigmoid')
])
# Compile the model
model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),
loss='binary_crossentropy',
metrics=['accuracy'])
# Train the model
history = model.fit(dataset.batch(batch_size), epochs=epochs, verbose=1)
# Plotting the training progress
plt.plot(history.history['loss'])
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()
